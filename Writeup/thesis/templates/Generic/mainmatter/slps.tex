\chapter{Heuristics for XOR counts}\label{ch:slp}
\openepigraph{\mbox{Premature optimization is the root of all evil}}{Donald Knuth}

\vspace*{-\baselineskip}
\newthought{Synopsis}\hspace{1.5em}
This chapter discusses heuristics for XOR count optimisations and the application of these to previously published results.
It is based on the article
\begin{quote}
    \fullfullcite{ToSC:KLSW17}.
\end{quote}
All authors contributed equally.

We start by recalling the notion of an \MDSs/ matrix and several special types of matrix constructions.
Then we examine the different XOR count metrics discussed in the literature and discuss heuristic approaches published in another line of research.
Finally we report on improvements on previously published results from the application of these heuristics.
We conclude the chapter with a statistical evaluation of different matrix constructions and a summary of follow up work.

\section{Introduction}
In the context of lightweight cryptography, several researchers started to tackle \crefName{prob:optimise_bb}, with a special focus on the linear layers more recently and even more specifically the implementation of \MDS/ matrices.
That is, linear layers with an optimal branch number.

The first line of work focused solely on minimising the chip area of the implementation.
This started with the block cipher \textsc{Present}~\cite{CHES:BKLPPR07} and goes over to many more designs, such as \textsc{Led}~\cite{CHES:GPPR11} and the hash function \textsc{Photon}~\cite{C:GuoPeyPos11}, where in the latter \MDS/ matrices were constructed that are especially optimised for chip area by allowing a serialised implementation.
However, there seem to be only a few practical applications where a small chip area is the only optimization goal and for those applications very good solutions are already available by now.

Later, starting with~\cite{CHES:KPPY14}, researchers focused on round-based implementations with the goal of finding \MDS/ constructions that minimise the number of XOR operations needed for their implementation.
Initially, the number of XOR operations needed was bounded by the number of ones in the binary representation of the matrix.

However, as the number of ones only gives an upper bound on the number of required XORs, several papers started to deviate from this conceptually easier but less accurate definition of XOR count and started to consider more efficient ways of implementing \MDS/ matrices.
Considering an $n\times n$ \MDS/ matrix over a finite field $\F_{2^k}$ given as $M=(m_{i,j})$ the aim was to choose the elements $m_{i,j}$ in such a way that implementing all of the multiplications $x \mapsto m_{i,j} x$ in parallel becomes as cheap as possible.
In order to compute the matrix $M$ entirely, those partial results have to be added together, for which an additional amount of XORs is required.
It became common to denote the former cost as the overhead and the later cost, \ie/ the cost of combining the partial results as a fixed, incompressible part.
A whole series of papers~\citeonly{FSE:SKOP15,C:BeiKraLea16,FSE:LiuSim16,FSE:LiWan16,AFRICACRYPT:SarSim16,ToSC:SarSye16,ToSC:JPST17,ToSC:LiWan17,ACISP:SarSye17,EPRINT:ZhoWanSun17} managed to reduce the overhead.

From a different viewpoint, what happened was that parts of the matrix, namely the cost of multiplication with the $m_{i,j}$, were extensively optimised, while taking the overall part of combining the parts as a given.
That is, researchers have focused on local optimization instead of global optimization.

Indeed the task of globally optimising is far from trivial, and thus the local optimization is a good step forward.

Interestingly, the task to optimise the cost of implementing the multiplication with a relatively large, \eg/, $32 \times 32$ binary matrix, is another extensively studied line of research.
It is known that the problem is NP-hard~\cite{JC:BoyMatPer13,MFCS:BoyMatPer08} and thus renders quickly infeasible for increasing matrix dimension.
However, quite a number of heuristic algorithms for finding the shortest linear straight-line program, which exactly corresponds to minimising the number of XORs, have been proposed in the literature~\citeonly{ISIT:Paar97,MFCS:BoyMatPer08,SEA:BoyPer10,IWIL:FuhSch10,SAT:FuhSch10,JC:BoyMatPer13,BFA:BoyFinPer17,EPRINT:VisSchPer17}.
Those algorithms produce very competitive results with a rather reasonable running time for arbitrary binary matrices of dimension up to at least 32.

Thus, the natural next step in order to optimise the cost of implementing \MDS/ matrices is to combine those two approaches.
This is exactly what we are doing in our work.

\newthoughtpar{Our contribution}
By applying the heuristic algorithms to find a short linear straight-line program to the case of \MDS/ matrices, we achieve the following.

First, we use several well-locally-optimised \MDS/ matrices from the literature and apply the known algorithms to all of them.
This is conceptually easy, with the main problem being the implementation of those algorithms.
In order to simplify this for follow-up works, we make our implementation publicly available.

This simple application leads immediately to significant improvements.
For instance, we get an implementation of the \AES/ MixColumn matrix that outperforms all implementations in the literature, \ie/ we use 97~XORs while the best implementation before used 103~XORs (\cite{CHES:JMPS17}).
In the case of applying it to the other constructions, we often get an implementation using \emph{less XOR operations than what was considered fixed costs before}.
That is, when (artificially) computing it, the overhead would actually be negative.
This confirms our intuition that the overhead was already very well optimised in previous work, such that now optimising globally is much more meaningful.

Second, we took a closer look at how the previous constructions compare when being globally optimised.
Interestingly, the previous best construction, \ie/ the \MDS/ matrix with smallest overhead, was most of the time \emph{not the one with the fewest XORs}.
Thus, with respect to the global optimum, the natural question was, which known construction actually performs best.
In order to analyze that, we did extensive experimental computations to compare the distribution of the optimised implementation cost for the various constructions.
The, somewhat disappointing, result is that all known constructions behave basically the same.
The one remarkable exception is the subfield construction for \MDS/ matrices, first introduced in \textsc{Whirlwind}~\cite{DCC:BNNRT10}.

Third, we looked at finding matrices that perform exceptionally well with respect to the global optimization, \ie/ which can be implemented with an exceptional low \emph{total} number of XORs.
Those results are summarised in \cref{slps:tab:best-mds}.
Compared to previous known matrices, ours improve on all -- with the exception of one, where the best known matrix is the already published matrix from~\cite{ToSC:SarSye16}.

Finally, we like to point out two restrictions of our approach.
First, we do not try to minimise the amount of temporary registers needed for the implementation.
Second, in line with all previous constructions, we do not minimise the circuit depth.
The later restriction is out of scope of the current work but certainly an interesting task for the future.

\section{MDS Matrices and Matrix Constructions}
\label{slps:sec:prelims}

\MDS/ matrices are related to a \MDSl/ code, hence their name, and  play an important role in the design of block ciphers.
From the diffusion point of view, such matrices are optimal and thus a natural choice for linear layers.
We recall now if and when a matrix is \MDS/ and what kind of constructions for \MDS/ matrices there are.
Additionally we discuss other popular constructions for matrices.

\subsection{\MDS/ matrices}
\label{slps:subsec:prelim1}

Following the importance of \MDS/ matrices for the diffusion in block cipher constructions, we define an \MDS/ matrix as follows.
\begin{definition}
\label{slps:def:MDS}
    An $n \times n$ matrix M is \emph{\MDSf/} if and only if $\mathrm{bn}(M) = n+1$.
\end{definition}
It has been shown that a matrix is \MDS/ if and only if all its square submatrices are invertible~\cite[page 321, Theorem 8]{error_correcting_codes}.
\MDS/ matrices do not exist for every choice of $n,k$.
The exact parameters for which \MDS/ matrices do or do not exist are investigated in the context of the famous \MDS/ conjecture which was initiated in~\cite{Segre1955}.

For binary matrices, we need to modify \cref{slps:def:MDS}.
\begin{definition}
    A binary matrix $B \in \F_2^{nk \times nk}$ is \emph{\MDS/} (for $k$-bit words) if and only if $\mathrm{bn}_k(M) = n+1$.
\end{definition}
\MDS/ matrices have a common application in linear layers of block ciphers, due to the wide trail strategy proposed for the \AES/, see~\cite{PhD:Daemen95,rijndael_book}.
We typically deal with $n \times n$ \MDS/ matrices over $\F_2^k$ respectively binary $\F_2^{nk \times nk}$ matrices that are \MDS/ for $k$-bit words where $k \in \{4,8\}$ is the size of the S-box.
In either case, when we call a matrix \MDS/, the size of $k$ will always be clear from the context when not explicitly mentioned.

It is easy to see that, if $M \in \F_{2^k}^{n \times n}$ is \MDS/, then also $\binarymatrix{M}$ is \MDS/ for $k$-bit words.
On the other hand, there might also exist binary \MDS/ matrices for $k$-bit words that do not have an according representation over~$\F_2^k$.

Other, non-\MDS/, matrices are also common in cipher designs.
To name only a few examples: \present/'s permutation matrix~\citeonly{CHES:BKLPPR07}, lightweight implementable matrices from \prince/~\citeonly{AC:BCGKKK12}, or \pride/~\citeonly{C:ADKLPY14}, or recently, \eg/ in \midori/~\citeonly{AC:BBISHA15}, or \qarma/~\citeonly{ToSC:Avanzi17}, almost-\MDS/ matrices.

\subsection{\MDS/ Constructions}
Cauchy and Vandermonde matrices are two famous constructions for building \MDS/ matrices.
They have the advantage of being provably \MDS/.

However, as known from the \MDS/ conjecture, for some parameter choices, \MDS/ matrices are unlikely to exist.
For example, we do not know how to construct \MDS/ matrices over $\F_{2^2}$ of dimension $4 \times 4$.

\begin{definition}[Cauchy matrix]
    Given two disjoint sets of $n$ elements of a field $\F_{2^k}$, $A = \set{a_1, \ldots, a_n}$, and $B = \set{b_1, \ldots, b_n}$.
    Then the matrix $M = \cauchy(A, B)$ consists of the entries $m_{i,j}$ with
    \begin{equation*}
        m_{i,j} \coloneqq \frac{1}{a_i - b_j}, \qquad \text{with}\ 1 \leq i,j \leq n
    \end{equation*}
    and is called a \emph{Cauchy} matrix.
\end{definition}

Every Cauchy matrix is \MDS/, \eg/ see~\cite[Lemma 1]{AFRICACRYPT:GupRay13}.

\begin{definition}[Vandermonde matrix]
    Given an $n$-tuple $(a_1,\ldots,a_n)$ with $a_i \in \F_{2^k}$.
    Then the matrix $M = \vandermonde(a_1, \ldots, a_n)$ is defined by the entries
    \begin{equation*}
        m_{i,j} \coloneqq a_i^{j-1}, \qquad \text{with}\ 1 \leq i,j \leq n
    \end{equation*}
    and is called \emph{Vandermonde} matrix.
\end{definition}

Given two Vandermonde matrices $A$ and $B$ with pairwise different $a_i$, $b_j$, then the matrix $AB^{-1}$ is \MDS/, see~\cite[Theorem 2]{CL:LacFim04}.
Furthermore, if $a_i = b_i + \Delta$ for all $i$ and an arbitrary nonzero $\Delta$, then the matrix $AB^{-1}$ is also involutory~\cite{CL:LacFim04,DCC:SDMO12}.

\subsection{Specially Structured Matrix Constructions}

Other constructions, such as circulant, Hadamard, or Toeplitz, are not per se \MDS/, but they have the advantage that they greatly reduce the search space by restricting the number of submatrices that appear in the matrix.
For circulant matrices, this was \eg/ already noted by \textcite{FSE:DaeKnuRij97}.

In order to generate a random \MDS/ matrix with one of these constructions, we can choose random elements for the matrix and then check for the \MDS/ condition.
Because of many repeated submatrices, the probability to find an \MDS/ matrix is much higher then for a fully random matrix.

\begin{definition}[Circulant matrices]
    A \emph{right circulant} $n \times n$ matrix is defined by the elements of its first row $a_1, \ldots, a_n$ as
    \begin{equation*}
        M = \circmatr_r(a_1, \ldots, a_n) \coloneqq \begin{pmatrix}
            a_1    & a_2    & \cdots & a_n     \\
            a_n    & a_1    & \cdots & a_{n-1} \\
            \vdots &        & \ddots & \vdots  \\
            a_2    & \cdots & a_n    & a_1     \\
        \end{pmatrix}.
    \end{equation*}
    A \emph{left circulant} $n \times n$ matrix is analogously defined as
    \begin{equation*}
        M = \circmatr_{\ell}(a_1, \ldots, a_n) \coloneqq \begin{pmatrix}
            a_1    & a_2    & \cdots & a_n     \\
            a_2    & a_3    & \cdots & a_1     \\
            \vdots &        & \ddots & \vdots  \\
            a_n    & a_1    & \cdots & a_{n-1} \\
        \end{pmatrix}.
    \end{equation*}
\end{definition}

While in the literature circulant matrices are almost always right circulant, left circulant matrices are equally fine for cryptographic applications.
The often noted advantage of right circulant matrices, the ability to implement the multiplication serialised and with shifts in order to save XORs, of course also applies to left circulant matrices.
Additionally, it is easy to see that the branch number of left and right circulant matrices are equal, $\mathrm{bn}(\circmatr{r}(a_1, \ldots, a_n)) = \mathrm{bn}(\circmatr_{\ell}(a_1, \ldots, a_n))$, since the matrices only differ in a permutation of the rows. Thus, for cryptographic purposes, it does not matter if a matrix is right circulant or left circulant and we will therefore simply talk about circulant matrices in general.
The common practice of restricting the matrix entries to elements from a finite field comes with the problem that circulant involutory \MDS/ matrices over finite fields do not exist, see~\cite{IJNSEC:NakAbr09}.
But \textcite{FSE:LiWan16} showed that this can be avoided by taking the matrix elements from the general linear group.

\begin{definition}[Hadamard matrix]
    A \emph{(finite field) Hadamard} matrix $M$ is of the form
    \begin{equation*}
        M = \begin{pmatrix} M_1 & M_2 \\ M_2 & M_1 \end{pmatrix},
    \end{equation*}
    where $M_1$ and $M_2$ are either Hadamard matrices themselves or one-dim\-en\-sio\-nal.
\end{definition}

The biggest advantage of Hadamard matrices is the possibility to construct involutory matrices.
If we choose the elements of our matrix such that the first row sums to one,\footnote{%
    Or to the identity matrix, if the elements are from the general linear group.
} the resulting matrix is involutory, see~\cite{AFRICACRYPT:GupRay13}.

\begin{definition}[Toeplitz matrix]
    An $n \times n$ \emph{Toeplitz} matrix $M$ is defined by the elements of its first row $a_1, \ldots, a_n$ and its first column $a_1, a_{n+1}, \ldots, a_{2n-1}$ as
    \begin{equation*}
        M = \toep(a_1, \ldots, a_n, a_{n+1}, \ldots, a_{2n-1}) \coloneqq \begin{pmatrix}
            a_1      & a_2      & \cdots & a_n     \\
            a_{n+1}  & a_1      & \ddots & a_{n-1} \\
            \vdots   & \ddots   & \ddots & \vdots  \\
            a_{2n-1} & a_{2n-2} & \cdots & a_1     \\
        \end{pmatrix},
    \end{equation*}
    that is, every element defines one minor diagonal of the matrix.
\end{definition}

To the best of our knowledge, \textcite{ToSC:SarSye16} were the first to scrutinise Toeplitz matrices in the context of XOR counts.

Finally, the subfield construction was first used to construct lightweight linear layers in the \whirlwind/ hash function~\cite[Section~2.2.2]{DCC:BNNRT10} and later used in~\citeonly{AFRICACRYPT:CYKGPP12,C:ADKLPY14,CHES:KPPY14,FSE:SKOP15,ToSC:JPST17}.
As its name suggests, the subfield construction was originally defined only for matrices over finite fields: a matrix with coefficients in $\F_{2^k}$ can be used to construct a matrix with coefficients in $\F_{2^{2k}}$.
Here, we use the natural extension to binary matrices.
\begin{definition}[Subfield matrix]
    \label{slps:def:subfield}
    Given an $n \times n$ matrix $M$ with entries $m_{i,j} \in \mathbb{F}_2^{k \times k}$.
    The subfield construction of $M$ is then an $n \times n$ matrix $M^\prime$ with
    \begin{equation*}
        M^\prime = \subfield(M) \coloneqq \parens{m_{i,j}^\prime},
    \end{equation*}
    where each $m_{i,j}^\prime = \parens{\begin{smallmatrix}m_{i,j} & 0 \\ 0 & m_{i,j}\end{smallmatrix}} \in \mathbb{F}_2^{2k \times 2k}$.
\end{definition}
In other words, for \cref{slps:def:subfield} we interpret the elements with which we multiply $M^\prime$ as elements of the product ring $\F_2^k \times \F_2^k$ and the linear layer acts as multiplying one copy of $M$ with the first parts of the input and another copy of $M$ with the second parts of the input.
This definition is straightforward to extend for more than one copy of the matrix $M$.

The subfield construction has some very useful properties, see~\citeonly{DCC:BNNRT10,CHES:KPPY14,FSE:SKOP15,ToSC:JPST17}.
\begin{lemma}\label{slps:lem:subfield}
    For the subfield construction, the following properties hold:
    \begin{enumerate}
        \item Let $M$ be a matrix that can be implemented with $m$ XORs.
            Then the matrix $M^\prime = \subfield(M)$ can be implemented with $2m$ XORs.
        \item Let $M$ be an \MDS/ matrix for $k$-bit words.
            Then $M^\prime = \subfield(M)$ is \MDS/ for $2k$-bit words.
        \item Let $M$ be an involutory matrix.
            Then $M^\prime = \subfield(M)$ is also involutory.
    \end{enumerate}
\end{lemma}
\begin{proof}
\begin{itemize}
\item[] \hphantom{.}
\item[(1)] Due to the special structure of the subfield construction, we can split the multiplication by $M^\prime$ into two multiplications by $M$, each on one half of the input bits.
           Hence, the XOR count doubles.
\item[(2)] We want to show that $\mathrm{hw}_{2k}(u) + \mathrm{hw}_{2k}(M^\prime u) \geq n+1$ for every nonzero $u$.
           We split $u$ into two parts $u_1$ and $u_2$, each containing alternating halves of the elements of $u$.
           As described in~\cite{CHES:KPPY14}, the multiplication of $M^\prime$ and $u$ is the same as the multiplication of the original matrix $M$ and each of the two $u_i$, if we combine the results according to our splitting.
           Let $t = \mathrm{hw}_{2k}(u) > 0$.
           Then, we have $t \geq \mathrm{hw}_k(u_1)$ and $t \geq \mathrm{hw}_k(u_2)$.
           Without loss of generality, let $\mathrm{hw}_k(u_1) > 0$.
           Since $M$ is \MDS/ for $k$-bit words, we have $\mathrm{hw}_k(M u_1) \geq n-t+1$ which directly leads to $\mathrm{hw}_{2k}(M^\prime u) \geq n-t+1$.
\item[(3)] As in the above proof, this property is straightforward to see.
           We want to show that $M^\prime M^\prime u = u$ for any vector $u$.
           Again, we split $u$ into two parts, $u_1$ and $u_2$, each containing alternating halves of the elements of $u$.
           Now, we need to show that $M M u_i = u_i$.
           This trivially holds, as $M$ is involutory.
\end{itemize}
\end{proof}

With respect to cryptographic designs, this means the following: assume we have an implementation with $m$ XORs for an (involutory) $n \times n$ \MDS/ matrix and $k$-bit S-boxes.
By applying the subfield construction we then easily find an implementation with $2m$ XORs for an (involutory) $n \times n$ \MDS/ matrix and $2k$-bit S-boxes.

We now turn to the XOR count metric and related work on it.

\section{XOR counts and Optimisation Strategies}

In 2014, \textcite{CHES:KPPY14} introduced the notion of XOR count as a metric to compare the area-efficiency of matrix multiplications.
Following that, there has been a lot of work~\citeonly{FSE:SKOP15,C:BeiKraLea16,FSE:LiuSim16,FSE:LiWan16,AFRICACRYPT:SarSim16,ToSC:SarSye16,ACISP:SarSye17,ToSC:JPST17,ToSC:LiWan17,EPRINT:ZhoWanSun17} on finding \MDS/ matrices that can be implemented with as few XOR gates as possible in the round-based scenario.

In an independent line of research, the problem of implementing binary matrix multiplications with as few XORs as possible was extensively studied~\citeonly{ISIT:Paar97,MFCS:BoyMatPer08,SEA:BoyPer10,IWIL:FuhSch10,SAT:FuhSch10,JC:BoyMatPer13,BFA:BoyFinPer17,EPRINT:VisSchPer17}.

In this section, we depict these two fields of research and show how they can be combined.

\subsection{The XOR count metric}
Let us first recall the scenario. In a round-based implementation the matrix is implemented as a fully unrolled circuit.
Thus, in the XOR count metric, the goal is to find a matrix that can be implemented with a circuit of as few (2-input) XOR gates as possible.
Basically every paper on this topic defines the XOR count as a property of the according matrix $M$.
But we will see in the remainder of this chapter that the actual XOR count highly depends on the \emph{implementation} of $M$.
To reflect this, we use the following definition.
\begin{definition}[XOR count]\label{def:xor-count}
    For a given a matrix $M$ denote by $\impl{M}$ an implementation that computes the map $x \mapsto M \cdot x$.
    The \emph{XOR count} of $\impl{M}$ is then defined as the number of XOR operations this implementation needs.
\end{definition}
Obviously, the implementation of a matrix, and thus the matrix' XOR count, is not unique.
If we want to highlight that an implementation was derived under some specific heuristic $H$, we denote this by $\implop_H(\cdot)$.
Of course, the matrix, or its implementation, has to fulfill some criteria, typically the matrix is \MDS/.
For finding matrices with a low XOR count, the question of how to create a circuit for a given matrix must be answered.

\subsection{Local Optimisations}
\label{slps:subsec:xor1}
The usual way for finding an implementation of $n \times n$ matrices over $\F_{2^k}$ was introduced in~\cite{CHES:KPPY14}.
As each of the $n$ output components of a matrix-vector multiplication is computed as a sum over $n$ products, the implementation is divided into two parts.
Namely the single multiplications on the one hand and addition of the products on the other hand.
As $\F_{2^k} \cong \F_2^k$, an addition of two elements from $\mathbb{F}_2^k$ requires $k$ XORs and thus adding up the products for all rows requires $n(n-1)k$ XORs in the case of an \MDS/ matrix where every element is nonzero.
If one implements the matrix like this, these $n(n-1)k$ XORs are a fixed part that cannot be changed.
Accordingly, many papers~\citeonly{C:BeiKraLea16,FSE:LiuSim16,FSE:LiWan16,EPRINT:ZhoWanSun17,ToSC:JPST17} just state the number of XORs for the single field multiplications when presenting results.
The other costs are regarded as inevitable.
The goal then boils down to constructing matrices with elements for which multiplication can be implemented with few XORs.
Thus, the original goal of finding a global implementation for the matrix is approached by locally looking at the single matrix elements.

To count the number of XORs for implementing a single multiplication with an element $\alpha \in \F_{2^k}$, the multiplication matrix $T_\alpha \in \F_2^{k \times k}$ is considered.
Such a matrix can be implemented in a straightforward way with $\mathrm{hw}(T_\alpha)-k$ XORs by simply implementing every XOR of the output components.
We call this the \emph{na\"ive} implementation of a matrix and when talking about the na\"ive XOR count of a matrix, we mean the $\mathrm{hw}(T_\alpha)-k$ XORs required for the na\"ive implementation.
In~\cite{ToSC:JPST17}, this is called d-XOR\@.
It is the easiest and most frequently used method of counting XORs.
Of course, in the same way we can also count the XORs of other matrices over $\F_2^{k \times k}$, \ie/ also matrices that were not originally defined over finite fields.

For improving the XOR count of the single multiplications, two methods have been introduced.
First, if the matrix is defined over some finite field, one can consider different field representations that lead to different multiplication matrices with potentially different Hamming weights, see~\citeonly{FSE:SKOP15,C:BeiKraLea16,AFRICACRYPT:SarSim16}.
Second, by reusing intermediate results, a $k \times k$ binary matrix might be implemented with less than $\mathrm{hw}(M)-k$ XORs, see~\citeonly{C:BeiKraLea16,ToSC:JPST17}.
In~\cite{ToSC:JPST17}, this is called s-XOR\@.
The according definitions from~\cite{ToSC:JPST17} and~\cite{C:BeiKraLea16} require that all operations must be carried out on the input registers.
If we want to compute an s-XOR count for a matrix, we are thus looking for an implementation that is constrained to use no temporary registers.
However, as we consider round-based hardware implementations, there is no need to avoid temporary registers since these are merely wires between gates.

Nowadays, the XOR count of implementations is mainly dominated by the $n(n-1)k$ XORs for putting together the locally optimised multiplications.
Lastly, we seem to hit a threshold and new results often improve existing results only by very few XORs.
The next natural step is to shift the focus from local optimisation of the single elements to the global optimisation of the whole matrix.
This was also formulated as future work in~\cite{ToSC:JPST17}.
As described in \cref{slps:sec:prelims}, we can use the binary representation to write an $n \times n$ matrix over $\F_{2^k}$ as a binary $nk \times nk$ matrix.
First we note, that the na\"ive XOR count of the binary representation is exactly the na\"ive XOR count of implementing each element multiplication and finally adding the results.
But if we look at the optimisation technique of reusing intermediate results for the whole $nk \times nk$ matrix, we now have many more degrees of freedom.
For the MixColumn matrix there already exists some work that goes beyond local optimisation.
An implementation with 108 XORs has been presented in~\citeonly{AC:SMTM01,INDOCRYPT:BanBogReg16,EPRINT:BanBogReg16b} and an implementation with 103 XORs in~\cite{CHES:JMPS17}.
A first step to a global optimisation algorithm was done in~\cite{EPRINT:ZWZZ16}.
However, their heuristic did not yield very good results and they finally had to go back to optimising submatrices.

Interestingly, much better algorithms for exactly this problem are already known from a different line of research.

\subsection{Global Optimisations}
\label{slps:subsec:xor2}
Implementing binary matrices with as few XOR operations as possible is also known as the problem of finding the \emph{shortest linear \SLP/}~\cite{SAT:FuhSch10,JC:BoyMatPer13} over the finite field with two elements.
Although this problem is NP-hard~\cite{MFCS:BoyMatPer08,JC:BoyMatPer13}, attempts have been made to find exact solutions for the minimum number of XORs.
Fuhs and Schneider-Kamp~\cite{IWIL:FuhSch10,SAT:FuhSch10} suggested to reduce the problem to satisfiability of Boolean logic.
They presented a general encoding scheme for deciding if a matrix can be implemented with a certain number of XORs.
Now, for finding the optimal implementation, they repeatedly use SAT solvers for a decreasing number of XORs.
Then, when they know that a matrix can be implemented with $\ell$ XORs, but cannot be implemented with $\ell - 1$ XORs, they are able to present $\ell$ as the optimal XOR count.
They used this technique to search for the minimum number of XORs necessary to compute a binary matrix of size $21 \times 8$, which is the first linear part of the \AES/ S-box, when it is decomposed into two linear parts and a minimal non-linear core.
While it worked to find a solution with 23 XORs and to show that no solution with 20 XORs exists, it turned out to be infeasible to prove that a solution with 22 XORs does not exist and that 23 is therefore the minimum.
In general, this approach quickly becomes infeasible for larger matrices.
\textcite{FSE:Stoffelen16} applied it successfully to a small $7 \times 7$ matrix, but did not manage to find a provably minimal solution with a specific matrix of size $19 \times 5$.
However, there do exist heuristics to efficiently find short linear \SLPp/ also for larger binary matrices.

Back in 1997, \textcite{ISIT:Paar97} studied how to optimise the arithmetic used by Reed-Solomon encoders.
Essentially, this boils down to reducing the number of XORs that are necessary for a constant multiplier over the field $\F_{2^k}$.
Paar described two algorithms that find a local optimum.
Intuitively, the idea of the algorithms is to iteratively eliminate \emph{common subexpressions}.
Let $T_{\alpha}$ be the multiplication matrix, to be applied to a variable field element $x = \left( x_1, \dotsc, x_k \right) \in \F_2^k$.
The first algorithm for computing $\impl{T_{\alpha}}$, denoted \textsc{Paar1} in the rest of this work, finds a pair $(i,j)$, with $i \neq j$, where the bitwise AND between columns $i$ and $j$ of $T_\alpha$ has the highest Hamming weight.
In other words, it finds a pair $(x_i, x_j)$ that occurs most frequently as subexpression in the output bits of $T_{\alpha}x$.
The XOR between those is then computed, and $T_{\alpha}$ and $\impl{T_{\alpha}}$ are updated accordingly, with $x_i + x_j$ as newly available variable.
This is repeated until there are no common subexpressions left.

The second algorithm, denoted \textsc{Paar2}, is similar, but differs when multiple pairs are equally common.
Instead of just taking the first pair, it recursively tries all of them.
The algorithm is therefore much slower, but can yield slightly improved results.
Compared to the na\"ive XOR count, Paar noted an average reduction in the number of XORs of 17.5\% for matrices over $\F_{2^4}$ and 40\% for matrices over $\F_{2^8}$.

In 2009,~\textcite{SPEEDCC:Ber09} presented an algorithm for efficiently implementing linear maps modulo 2.
Based on this and on~\cite{ISIT:Paar97}, a new algorithm was presented in~\cite{SAC:BerCho14}.
However, the algorithms from~\cite{SPEEDCC:Ber09,SAC:BerCho14} have a different framework in mind and yield a higher number of XORs compared to~\cite{ISIT:Paar97}.

Paar's algorithms lead to so-called \emph{cancellation-free} implementations. This means that for every XOR operation $u + v$, none of the input bit variables $x_i$ occurs in both $u$ and $v$.
Thus, the possibility that two variables cancel each other out is never taken into consideration, while this may in fact yield a more efficient solution in terms of the total number of XORs.
In 2008, \textcite{MFCS:BoyMatPer08} showed that cancellation-free techniques can often not be expected to yield optimal solutions for non-trivial inputs.
They also showed that, even under the restriction to cancellation-free programs, the problem of finding an optimal \SLP/ is NP-complete.

Around 2010, \textcite{SEA:BoyPer10} came up with a heuristic that is not cancellation-free and that improved on Paar's algorithms in most scenarios.
Their idea was to keep track of a distance vector that contains, for each targeted expression of an output bit, the minimum number of additions of the already computed intermediate values that are necessary to obtain that target.
To decide which values will be added, the pair that minimises the sum of new distances is picked.
If there is a tie, the pair that maximises the Euclidean norm of the new distances is chosen.
Additionally, if the addition of two values immediately leads to a targeted output, this can always be done without searching further.
This algorithm works very well in practice, although it is slower compared to \textsc{Paar1}.

Next to using the Euclidean norm as tie breaker, they also experimented with alternative criteria.
For example, choosing the pair that maximises the Euclidean norm minus the largest distance, or minus the difference between the two largest distances.
The results were then actually very similar.
Another tie-breaking method is to flip a coin and choose a pair randomly.
The algorithm is now no longer deterministic and can be run multiple times.
The lowest result can then be used.
This performed slightly better, but of course processing again takes longer.

The results of~\cite{MFCS:BoyMatPer08,SEA:BoyPer10} were later improved and published in the journal of cryptology~\cite{JC:BoyMatPer13}.

In early 2017, \textcite{EPRINT:VisSchPer17} explored the special case where the binary matrix is dense.
They improved the heuristic on average for dense matrices by first computing a \emph{common path}, an intermediate value that contains most variables.
The original algorithm then starts from this common path.
At BFA 2017, \citeauthor{BFA:BoyFinPer17} presented an improvement that simultaneously reduces the number of XORs and the depth of the resulting circuit, see~\cite{BFA:BoyFinPer17}.

We refer to this family of heuristics~\cite{MFCS:BoyMatPer08,SEA:BoyPer10,JC:BoyMatPer13,BFA:BoyFinPer17,EPRINT:VisSchPer17} as the \textsc{BP} heuristics.

\section{Application and Evaluation of the Heuristics}
\label{slps:sec:results}

Using the techniques described above, we now give XOR counts for optimised implementations of published matrices.
Next, we analyze the statistical behaviour of different matrix constructions.
Finally we summarise the to date best known matrices.

\subsection{Improved Implementations of Matrices}

Using the heuristic methods that are described in the previous section, we can easily and significantly reduce the XOR counts for many matrices that have been used in the literature.
The running times for the optimisations are in the range of seconds to minutes.
\Cref{slps:tab:mds_comparison} at the end of this chapter lists results for matrices that have been suggested in previous works where it was an explicit goal to find a lightweight \MDS/ matrix.
While the constructions themselves will be compared in \cref{slps:sec:comparison}, this table deals with the suggested instances.

A number of issues arise from this that are worth highlighting.
First of all, it should be noted that without any exception, the XOR count for every matrix could be reduced with little effort.
Second, it turns out that there are many cases where the $n(n-1)k$ XORs for summing the products for all rows is not even a correct lower bound.
In fact, all the $4 \times 4$ matrices over $\GL{4}$ that we studied can be implemented in \emph{at most} $4 \cdot 3 \cdot 4 = 48$ XORs.

What may be more interesting, is whether the XOR count, as it was used previously, is in fact a good predictor for the actual implementation cost as given by the heuristic methods.
Here we see that there are some differences.
For example,~\cite{FSE:LiWan16}'s circulant $4 \times 4$ matrices over $\GL{8}$ first compared very favorably, but we now find that the subfield matrix of~\cite{ToSC:JPST17} requires fewer XORs.

Regarding involutory matrices, it was typically the case that there was an extra cost involved to meet this additional criterion.
However, the heuristics sometimes find implementations with even fewer XORs than the non-involutory matrix that was suggested.
See for example the matrices of~\cite{ToSC:SarSye16} in the table.

Aside from these matrices, we also looked at \MDS/ matrices that are used by various ciphers and hash functions.
\Cref{slps:tab:ciphers} lists their results, it can be found at the end of this chapter, too.
Not all \MDS/ matrices that are used in ciphers are incorporated here.
In particular, \led/~\cite{CHES:GPPR11}, \photon/~\cite{C:GuoPeyPos11}, and \primates/~\cite{CAESAR:primates} use efficient serialised \MDS/ matrices.
Comparing these to our \enquote{unrolled} implementations would be somewhat unfair.

The implementation of the \MDS/ matrix used in \AES/ with 97 XORs is, to the best of our knowledge, the most efficient implementation so far and improves on the previous implementation of 103 XORs, reported by~\citeonly{CHES:JMPS17}.
As a side note, cancellations do occur in this implementation, we thus conjecture that such a low XOR count is not possible with cancellation-free programs.

\subsection{Statistical Analysis}
\label{slps:sec:comparison}
Several constructions for building \MDS/ matrices are known.
But it is not clear which one is the best when we want to construct matrices which can yield an implementation with a low XOR count.
In this section, we present experimental results on different constructions and draw conclusions for the designer.
We also examine the correlation between na\"ive and heuristically improved XOR counts.
When designing \MDS/ matrices with a low XOR count, we are faced with two major questions.
First, which construction is preferable?
Our intuition in this case is a better construction has better statistical properties compared to an inferior construction.
We are aware that the statistical behaviour of a construction might not be very important for a designer who only looks for a single, very good instance.
Nevertheless we use this as a first benchmark.
Second, is it a good approach to choose the matrices as sparse as possible?
In order to compare the listed constructions, we construct random instances of each and then analyze their implementations with statistical means.

Building Cauchy and Vandermonde matrices is straightforward as we only need to choose the defining elements randomly from the underlying field.
For the other constructions, we use the following backtracking method to build random \MDS/ constructions of dimension $4 \times 4$.
Choose the new random elements from $\mathrm{GL}(k, \F_2)$ that are needed for the matrix construction in a step-by-step manner.
In each step, construct all new square submatrices. If any of these is not invertible, discard the chosen element and try a new one. In the case that no more elements are left, go one step back and replace that element with a new one, then again check the according square submatrices, and so on.
Eventually, we end up with an \MDS/ matrix because we iteratively checked that every square submatrix is invertible.
The method is also trivially derandomisable, by not choosing the elements randomly, but simply enumerating them in any determined order.

Apart from applying this method to the above mentioned constructions, we can also use it to construct an \emph{arbitrary}, \ie/ unstructured, matrix that is simply defined by its $16$ elements.
This approach was already described in~\cite{ToSC:JPST17}.

In this manner, we generated 1\,000 matrices for each construction and computed the XOR count distributions for the na\"ive, the optimised \textsc{Paar1}, and \textsc{BP} implementations.
\Cref{slps:tab:stats} lists the statistical parameters of the resulting distributions and \cref{slps:fig:distributions} (at the end of the chapter) depicts them (the sample size $N$ is the same for \cref{slps:tab:stats}, \cref{slps:fig:distributions,slps:fig:hadamard_correlation}, and
\cite[Appendix~A, Figures~3 to~6]{ToSC:KLSW17}%
%\cref{slps:fig:correlations_naive_paar1_random,slps:fig:correlations_naive_paar1_enum,slps:fig:correlations_naive_bp_random,slps:fig:correlations_naive_bp_enum}
).

\begin{table}
    \begin{sidecaption}[XOR count distributions for optimised implementations]{%
        Distributions for differently optimised implementations.
        By $\mu$ we denote the mean and $\sigma^2$ is the variance.
        The sample size $N$ for all distributions is $N = 1000$.
    }[slps:tab:stats]
    \centering
    \begin{threeparttable}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lrrcrrcrr}
        \toprule
                          & \multicolumn{2}{c}{Na\"ive} & & \multicolumn{2}{c}{\textsc{Paar1}} & & \multicolumn{2}{c}{\textsc{BP}} \\
        Construction      & $\mu$ &$\sigma^2$& & $\mu$ &$\sigma^2$& &$\mu$&$\sigma^2$\\
        \midrule
        \multicolumn{9}{c}{$4 \times 4$ matrices over $\GL{4}$}                      \\ \rowcolor{gray!10}
        \midrule
        Cauchy            & 120.7 &   77.3   & &  62.9 &   11.0  & & 53.1 &    4.0   \\
        Circulant         & 111.8 &  117.1   & &  60.4 &   19.2  & & 52.1 &    7.1   \\ \rowcolor{gray!10}
        Hadamard          & 117.5 &   99.6   & &  60.2 &   17.8  & & 52.4 &    6.9   \\
        Toeplitz          & 112.8 &   39.9   & &  59.9 &    7.4  & & 51.3 &    3.9   \\ \rowcolor{gray!10}
        Vandermonde       & 120.6 &   87.6   & &  62.2 &    8.1  & & 52.9 &    3.1   \\
        \bottomrule
        \toprule
        \multicolumn{9}{c}{enumerated $4 \times 4$ matrices over $\GL{4}$}           \\ \rowcolor{gray!10}
        \midrule
        Circulant         &  82.9 &   53.0   & &  54.9 &   13.5  & & 50.1 &    6.7   \\
        Hadamard          & 102.1 &   76.0   & &  56.7 &   20.6  & & 50.6 &    8.0   \\ \rowcolor{gray!10}
        Toeplitz          &  86.1 &   43.9   & &  55.3 &    8.3  & & 49.4 &    3.9   \\
        Arbitrary         &  80.5 &    8.3   & &  49.7 &    3.2  & & 44.5 &    1.8   \\
        \bottomrule
        \toprule
        \multicolumn{9}{c}{$4 \times 4$ matrices over $\GL{8}$}                      \\ \rowcolor{gray!10}
        \midrule
        Cauchy            & 454.1 &  467.2   & & 215.1 &   39.6  & & ---  &   ---    \\
        Vandermonde       & 487.3 &  597.4   & & 220.2 &   44.3  & & ---  &   ---    \\
        \bottomrule
        \toprule
        \multicolumn{9}{c}{$4 \times 4$ subfield matrices over $\GL{4}$}             \\ \rowcolor{gray!10}
        \midrule
        Cauchy            & 241.1 &  312.1   & & 125.8 &   44.2  & & ---  &   ---    \\
        Vandermonde       & 240.6 &  452.8   & & 121.8 &   47.1  & & ---  &   ---    \\
        \bottomrule
    \end{tabular}
    %\begin{tablenotes}
    %\end{tablenotes}
    \end{threeparttable}
    \end{sidecaption}
\end{table}

The most obvious characteristic of the statistical distributions is that the means~$\mu$ do not differ much for all randomised constructions.
The variance~$\sigma^2$ on the contrary differs much more.
This is most noticeable for the na\"ive implementation, while the differences get much smaller when the implementation is optimised with the \textsc{Paar1} or \textsc{BP} heuristic.
One might think that the construction with the lowest optimised average XOR count, which is for matrices over $\mathrm{GL}(4, \mathbb{F}_2)$ the arbitrary construction with enumerated elements, yields the best results.
However, the best matrix we could find for this dimension was a Hadamard matrix.
An explanation for this might be that the higher variance leads to some particularly bad and some particularly good results.

The graphs in \cref{slps:fig:distributions} convey a similar hypothesis.
Looking only at the na\"ive implementation, we can notice some differences.
For example circulant matrices seem to give better results than, \eg/, Hadamard matrices.
Additionally, the na\"ive XOR count increases step-wise as not every possible count occurs.
But when optimising the implementation, the distributions get smoother and more similar.

We conclude that all constructions give similarly good matrices when we are searching for the matrix with the lowest XOR count, with one important exception.
For randomly generated matrices the XOR count increases by a factor of four, if we double the parameter $k$.
\Cref{slps:tab:stats} covers this for Cauchy and Vandermonde matrices.
We do not compute the statistical properties for Circulant, Hadamard and Toeplitz matrices with elements of $\mathrm{GL}(8, \F_2)$, as the probability to find a random \MDS/ instance for these constructions is quite low.
Thus, generating enough instances for a meaningful statistical comparison is computationally tough and -- as we deduce from a much smaller sample size -- the statistical behaviour looks very similar to that of the Cauchy and Vandermonde matrices.
Instead, and as already mentioned in \cref{slps:lem:subfield}, the subfield construction has a much more interesting behaviour.
Its implementation simply doubles the XOR count.
The lower half of \cref{slps:tab:stats} confirms this behaviour.

Thus, when it is computationally infeasible to exhaustively search through all possible matrices, it seems to be a good strategy to use the subfield construction with the best known results from smaller dimensions.
This conclusion is confirmed by the fact that our best results for matrices over $\mathrm{GL}(8, \mathbb{F}_2)$ are always subfield constructions based on matrices over $\mathrm{GL}(4, \mathbb{F}_2)$.

Next, we want to approach the question if choosing \MDS/ matrices with low Hamming weight entries is a good approach for finding low XOR count implementations.
To give a first intuition of the correlation between na\"ive and optimised implementations, we plot the na\"ive XOR count against the optimised one.
For one exemplary plot see \cref{slps:fig:hadamard_correlation}, which corresponds to the construction that we used to find the best $4 \times 4$ \MDS/ matrix for $k = 4$.
The remaining plots
can be found in the paper, see~\cite[Appendix~A, Figures~3 to~6]{ToSC:KLSW17}.
%are in the appendix, see \cref{slps:fig:correlations_naive_paar1_random,slps:fig:correlations_naive_paar1_enum,slps:fig:correlations_naive_bp_random,slps:fig:correlations_naive_bp_enum}.

%\begin{figure}
\marginpar{%
    \centering
    \vspace*{-16\baselineskip}
    \begin{tikzpicture}
        \pgfplotsset{%
            legend style={font=\footnotesize, draw=none, fill=none},
            label style={font=\footnotesize},
            every tick label/.append style={font=\footnotesize},
        }
        \begin{axis}[height=55mm, width=55mm, ymin=35, ymax=60, xmin=65, xmax=120, xlabel={Na\"ive}, ylabel={BP}]
            \addplot+[only marks, mark size=0.5pt] table[x index=1, y index=3] {slp/enum_hadamard_4x4_4.dat};
        \end{axis}
    \end{tikzpicture}
    \captionof{figure}[Correlations between na\"ive and \textsc{BP} XOR counts]{%
        Correlations between na\"ive and \textsc{BP} XOR counts for enumerated Hadamard matrices.
    }\label{slps:fig:hadamard_correlation}
%    \begin{sidecaption}[Correlations between na\"ive and \textsc{BP} XOR counts]{%
%        Correlations between na\"ive (x-axis) and \textsc{BP} (y-axis) XOR counts for enumerated Hadamard matrices.
%    }[slps:fig:hadamard_correlation]
%    \end{sidecaption}
%\end{figure}
}

In \cref{slps:fig:hadamard_correlation} one can see that several options can occur.
While there is a general tendency of higher na\"ive XOR counts leading to higher optimised XOR counts, the contrary is also possible.
For example, there are matrices which have a low na\"ive XOR count (left in the plot), while still having a somewhat high optimised XOR count (top part of the plot).
But there are also matrices where a higher na\"ive XOR count results in a much better optimised XOR count.
The consequence is that we cannot restrict ourselves to very sparse matrices when searching for the best XOR count, but also have to take more dense matrices into account.
A possible explanation for this behaviour is that the heuristics have more possibilities for optimisations, when the matrix is not sparse.

\begin{table}[t]
    \begin{sidecaption}[\MDSs/ matrices with lowest known XOR count]{%
        \MDSs/ matrices with the lowest known XOR count.
        Matrices in the lower half are involutory.
    }[slps:tab:best-mds]
    \centering
    {
    \begin{threeparttable}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lrlrl}
        \toprule
        Type                                           & \multicolumn{2}{c}{Previously Best Known}    &    \multicolumn{2}{c}{New Best Known}                      \\
        \midrule
        ${\mathrm{GL}(4, \F_2)}^{4 \times 4}$          &  $58$ & \citeonly{ToSC:SarSye16,ToSC:JPST17} &  36\tnote{*}    & Eq.~\eqref{slps:eq:mds_matrix_4x4_4}     \\ \rowcolor{gray!10}
        ${\mathrm{GL}(8, \F_2)}^{4 \times 4}$          & $106$ & \citeonly{FSE:LiWan16}               &  72             & Eq.~\eqref{slps:eq:mds_matrix_4x4_8}     \\
        $\parens{\F_2[x]/\mathtt{0x 13}}^{8 \times 8}$ & $384$ & \citeonly{FSE:SKOP15}                & 196\tnote{\dag} & Eq.~\eqref{slps:eq:mds_matrix_8x8_4}     \\ \rowcolor{gray!10}
        ${\mathrm{GL}(8, \F_2)}^{8 \times 8}$          & $640$ & \citeonly{FSE:LiuSim16}              & 392             & Eq.~\eqref{slps:eq:mds_matrix_8x8_8}     \\
        \midrule
        $\parens{\F_2[x]/\mathtt{0x 13}}^{4 \times 4}$ &  $63$ & \citeonly{ToSC:JPST17}               &  42\tnote{*}    & \citeonly{ToSC:SarSye16}                 \\ \rowcolor{gray!10}
        ${\mathrm{GL}(8, \F_2)}^{4 \times 4}$          & $126$ & \citeonly{ToSC:JPST17}               &  84             & Eq.~\eqref{slps:eq:mds_inv_matrix_4x4_8} \\
        $\parens{\F_2[x]/\mathtt{0x 13}}^{8 \times 8}$ & $424$ & \citeonly{FSE:SKOP15}                & 212\tnote{\dag} & Eq.~\eqref{slps:eq:mds_inv_matrix_8x8_4} \\ \rowcolor{gray!10}
        ${\mathrm{GL}(8, \F_2)}^{8 \times 8}$          & $736$ & \citeonly{ToSC:JPST17}               & 424             & Eq.~\eqref{slps:eq:mds_inv_matrix_8x8_8} \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item[*] Computed with heuristic from~\citeonly{JC:BoyMatPer13}.
    \item[\dag] Computed with heuristic from~\citeonly{ISIT:Paar97}.
    \end{tablenotes}
    \end{threeparttable}
    }
    \end{sidecaption}
\end{table}

\subsection{Best results}
Let us conclude by specifying the \MDS/ matrices with the currently best implementations.
The notation $M_{n,k}$ denotes an $n \times n$ matrix with entries from $\GL{k}$, an involutory matrix is labeled with the superscript $i$.
\Cref{slps:tab:best-mds} covers non-involutory and involutory matrices of dimension $4 \times 4$ and $8 \times 8$ over $\GL{4}$ and $\GL{8}$.
$M_{8,4}$ and $M^i_{8,4}$ are defined over $\F_2[x] /\mathtt{0x13}$.

The matrices mentioned are the following:
\begin{align}
    M_{4,4} &= \hadamard(\parens{\begin{smallmatrix}
        0 & 0 & 0 & 1 \\
        0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0 \\
    \end{smallmatrix}},
    \parens{\begin{smallmatrix}
        0 & 0 & 1 & 1 \\
        1 & 0 & 0 & 1 \\
        1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
    \end{smallmatrix}},
    \parens{\begin{smallmatrix}
        1 & 1 & 0 & 1 \\
        1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 \\
        0 & 0 & 1 & 0 \\
    \end{smallmatrix}},
    \parens{\begin{smallmatrix}
        1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 \\
        1 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 \\
    \end{smallmatrix}}) \label{slps:eq:mds_matrix_4x4_4} \\
    M_{4,8} &= \subfield(M_{4,4}) \label{slps:eq:mds_matrix_4x4_8} \\
    M_{8,4} &= \cauchy\parens{%
        \begin{multlined}
            \scriptstyle
            x^3 + x^2, x^3, x^3 + x + 1, x + 1, 0, x^3 + x^2 + x + 1, x^2, x^2 + x + 1, \\
            \scriptstyle
            1, x^2 + 1, x^3 + x^2 + x, x^3 + 1, x^3 + x^2 + 1, x^2 + x, x^3 + x, x
        \end{multlined}}\label{slps:eq:mds_matrix_8x8_4} \\
    M_{8,8} &= \subfield(M_{8,4}) \label{slps:eq:mds_matrix_8x8_8} \\
    M^i_{4,8} &\ \text{is the subfield construction applied to~\citeonly[Example~3]{ToSC:SarSye16}} \label{slps:eq:mds_inv_matrix_4x4_8} \\
    M^i_{8,4} &= \vandermonde\parens{%
        \begin{multlined}
            \scriptstyle
            x^3 + x + 1, x + 1, x^3 + x^2 + x, x^3 + x^2 + 1, x^3 + 1, x^3, 0, x^3 + x \\
            \scriptstyle
            x^2 + x + 1, x^3 + x^2 + x + 1, x, 1, x^2 + 1, x^2, x^3 + x^2, x^2 + x
        \end{multlined}}\label{slps:eq:mds_inv_matrix_8x8_4} \\
    M^i_{8,8} &= \subfield(M^i_{8,4}) \label{slps:eq:mds_inv_matrix_8x8_8}
\end{align}

All these matrices improve over the previously known matrices, with the only exception being the involutory matrix from~\cite{ToSC:SarSye16} of dimension $4 \times 4$ over $\GL{4}$.
$M_{4,4}$ was found after enumerating a few thousand Hadamard matrices, while $M_{8,4}$ and $M^i_{8,4}$ are randomly generated and were found after a few seconds.
Every best matrix over $\GL{8}$ uses the subfield construction.

With these results we want to highlight that, when applying global optimisations, it is quite easy to improve (almost) all currently best known results.
We would like to emphasise that our results should not be misunderstood as an attempt to construct matrices which cannot be improved.

All our implementations are publicly available online.\footnote{
    \url{https://github.com/rub-hgi/shorter_linear_slps_for_mds_matrices}
}

\section{Further work}
Our work mainly highlights that in order to get a tighter estimate on how few XORs are really needed for a given matrix $M$, we have to take optimisation strategies for its implementation into account.
As already mentioned, this implementation can be subject to several constraints which we did not further investigate.
One important aspect in this direction is the depth of the critical path of an implementation.
The depth is an important factor for the latency of an implementation and thus subject to a different optimisation goal.
There is some related work, which already takes the depth of the critical path into consideration, \eg/~\cite{BFA:BoyFinPer17}.

A recent work by \textcite{ToSC:DuvLeu18} also compares proposed matrices with implementation depth in mind.
Additionally, \citeauthor{ToSC:DuvLeu18} propose a graph-based search algorithm to find optimal implementations that yield an \MDS/ matrix.
Using their algorithm, the authors are able to find matrices with cheaper and less deep implementations.
Especially when taking the circuit depth into account, their approach is obviously favourable to ours.
For example, the best implementations for $4 \times 4$ matrices over $\mathrm{GL}(4, \F_2)$ of~\cite{ToSC:DuvLeu18} need 35, 37, resp.\ 41 XORs (with a depth of five, four, resp.\ three), while our implementation of $M_{4,4}$ has depth six, see~\cite[Table~1]{ToSC:DuvLeu18}.

An interesting observation is that their implementations can be further optimised by synthesis tools like Yosys\footnote{%
    \url{http://www.clifford.at/yosys}
} -- this indicates, and is also discussed by the authors, that the XOR count is not necessarily a good metric of the actual implementation cost in silicon.
The example \citeauthor{ToSC:DuvLeu18} give are \FPGAp/, where the XORs are implemented using internal programmable \LUTp/.
These \LUTp/ can implement not only 2-input XORs, but also 3- or 4-input or even 6- or 8-input XORs with almost no overhead compared to a simple 2-input XOR.
This can of course greatly improve existing implementations.
\citeauthor{ToSC:DuvLeu18} further argue that the costs for different XOR gates in \ASIC/ synthesis differ more and thus not so big improvements can be expected.

In our opinion, this is not true.
Standard cell libraries for \ASIC/ design often offers 2-input and 3-input XORs and the area cost for 3-input XOR gates might be cheap enough that it pays out to use these gates in an optimised implementation.
For example in the \texttt{UMCL18G212T3} library, see \eg/~\cite[Table~2.1]{PhD:Poschmann09}, the 2-input XOR costs 2.67 GE (gate equivalents), while a 3-input XOR gate costs 4.67 GE which is $\approx 87\%$ of the cost for two 2-input XOR gates.
A recent work by \textcite{IWSEC:BanFunIso19} explores these differences; one of their main findings is that different matrices are optimal for different libraries.

The observation that it is crucial to take library gate costs into account was also already made by \textcite{TCHES:ReyTahAsh18} in the context of S-box implementation optimisations.
Here the authors noted that the theoretical best implementations are in practice \emph{worse} than less optimised implementations -- due to the fact that the less optimised implementation uses cheaper standard cell gates than the optimised ones.
\citeauthor{TCHES:ReyTahAsh18} thus suggest that either directly the synthesis tools or the information these tools use during synthesis, that is the individual costs and characteristics (\eg/ delay) of the available gates, have to be included in the theoretical optimisation process.

The more problematic (obvious) point in our opinion then is that the resulting implementations are highly dependent on the targeted cell library and, see~\cite{IWSEC:BanFunIso19}, different matrices are optimal for different cell libraries.
On the other hand it is questionable how problematic it is if the area requirements for a matrix implementation differs by a handful of gate equivalents.

An other interesting research question is how the existing heuristics can be improved.
The already mentioned work by \textcite{TCHES:ReyTahAsh18} proposes some improvements for the BP heuristics, but evaluate their improvements only in the setting of S-box optimisations.
\textcite{ToSC:LSLWH19} also improves the BP heuristics, to make them depth aware.
They are also able to find new involutory matrices with cheaper implementations.

%We now turn away from automated methods in the area of implementations and discuss applications in the area of cryptanalysis.

\begin{table}
    \begin{sidecaption}[Comparison of $4 \times 4$ and $8 \times 8$ \MDSs/ matrices]{%
        Comparison of $4 \times 4$ and $8 \times 8$ \MDSs/ matrices over $\GL{4}$ and $\GL{8}$.
        Bold entries in the Literature column are matrices with up to now lowest XOR counts, bold entries in the BP column are the matrices with now lowest XOR count.
        The matrix constructions are Circulant~(C), Hadamard~(H), Involutory~(I), Subfield~(S), and Toeplitz~(T).
    }[slps:tab:mds_comparison]
    \centering
    \begin{threeparttable}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lrrrrr}
        \toprule
        Matrix                                           & Na\"ive & Literature & \textsc{Paar1} &    \textsc{Paar2} & \textsc{BP} \\
        \midrule
        \multicolumn{6}{c}{$4 \times 4$ matrices over $\GL{4}$}                                                  \\
        \midrule
        \citeonly{FSE:SKOP15} (H)                 &  68   &  $20+48$   &      50        &       48\tnote{*} &  48 \\ \rowcolor{gray!10}
        \citeonly{FSE:LiuSim16} (C)               &  60   &  $12+48$   &      49        &       46\tnote{*} &  44 \\
        \citeonly{FSE:LiWan16} (C)\tnote{\dag}    &  60   &  $12+48$   &      48        &       47\tnote{*} &  44 \\ \rowcolor{gray!10}
        \citeonly{C:BeiKraLea16} (C)\tnote{\dag}  &  64   &  $12+48$   &      48        &       47          &  \textbf{42} \\
        \citeonly{ToSC:SarSye16} (T)              &  58   &  $\mathbf{10+48}$   &      46        &       45\tnote{*} &  43 \\ \rowcolor{gray!10}
        \citeonly{ToSC:JPST17}                    &  61   &  $\mathbf{10+48}$   &      48        &       47          &  43 \\
        \midrule
        \citeonly{FSE:SKOP15} (H, I)              &  72   &  $24+48$   &      52        &       48\tnote{*} &  48 \\ \rowcolor{gray!10}
        \citeonly{FSE:LiWan16} (C, I)             &  68   &  $20+48$   &      48        &       48          &  48 \\
        \citeonly{ToSC:SarSye16} (I)              &  64   &  $16+48$   &      50        &       48          &  \textbf{42} \\ \rowcolor{gray!10}
        \citeonly{ToSC:JPST17} (I)                &  68   &  $\mathbf{15+48}$   &      51        &       47\tnote{*} &  47 \\ %\rowcolor{gray!10}
        \bottomrule
        \toprule
        \multicolumn{6}{c}{$4 \times 4$ matrices over $\GL{8}$}                                                  \\
        \midrule
        \citeonly{FSE:SKOP15} (S)                 &  136  & $40+96$ & 100       &      98\tnote{*}  & 100 \\ \rowcolor{gray!10}
        \citeonly{FSE:LiuSim16} (C)               &  128  & $28+96$\tnote{1} & 116       &     116           & 112 \\
        \citeonly{FSE:LiWan16}                            &  106  & $\mathbf{10+96}$ & 102       &     102           & 102 \\ \rowcolor{gray!10}
        \citeonly{C:BeiKraLea16} (C)              &  136  & $24+96$ & 116       &     112\tnote{*}  & 110 \\
        \citeonly{ToSC:SarSye16} (T)              &  123  & $24+96$\tnote{1} & 110       &     108           & 107 \\ \rowcolor{gray!10}
        \citeonly{ToSC:JPST17} (S)                &  122  & $20+96$ &  96       &      95\tnote{*}  &  \textbf{86} \\
        \midrule
        \citeonly{FSE:SKOP15} (S, I)              &  144  & $40+96$\tnote{1} & 104       &     101\tnote{*}  & 100 \\ \rowcolor{gray!10}
        \citeonly{FSE:LiWan16} (H, I)             &  136  & $40+96$ & 101       &      97\tnote{*}  &  \textbf{91} \\
        \citeonly{ToSC:SarSye16} (I)              &  160  & $64+96$ & 110       &     109\tnote{*}  & 100 \\ \rowcolor{gray!10}
        \citeonly{ToSC:JPST17} (S, I)             &  136  & $\mathbf{30+96}$ & 102       &     100\tnote{*}  &  \textbf{91} \\ %\rowcolor{gray!10}
        \bottomrule
        \toprule
        \multicolumn{6}{c}{$8 \times 8$ matrices over $\GL{4}$}                                                  \\
        \midrule
        \citeonly{FSE:SKOP15} (H)                 &  432  & $\mathbf{160+224}\tnote{1}$ & 210    &     209\tnote{*}  & \textbf{194} \\ \rowcolor{gray!10}
        \citeonly{ACISP:SarSye17} (T)             &  410  & $170+224$          & 212    &     212\tnote{*}  & 204 \\
        \midrule
        \citeonly{FSE:SKOP15} (H, I)              &  512  & $\mathbf{200+224}\tnote{1}$ & 222    &     222\tnote{*}  & \textbf{217} \\
        \bottomrule
        \toprule
        \multicolumn{6}{c}{$8 \times 8$ matrices over $\GL{8}$}                                                  \\ \rowcolor{gray!10}
        \midrule
        \citeonly{FSE:SKOP15} (H)                 &  768  & $256+448\tnote{1}$ & 474    &     ---           & 467 \\
        \citeonly{FSE:LiuSim16} (C)               &  688  & $\mathbf{192+448}\tnote{1}$ & 464    &     ---           & 447 \\ \rowcolor{gray!10}
        \citeonly{C:BeiKraLea16} (C)              &  784  & $208+448\tnote{1}$ & 506    &     ---           & 498 \\
        \citeonly{ACISP:SarSye17} (T)             &  680  & $232+448$          & 447    &     ---           & \textbf{438} \\ \rowcolor{gray!10}
        \midrule
        \citeonly{FSE:SKOP15} (H, I)              &  816  & $320+448\tnote{1}$ & 430    &     ---           & \textbf{428} \\
        \citeonly{ToSC:JPST17} (H, I)             & 1152  & $\mathbf{288+448}$ & 620    &     ---           & 599 \\
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item[*] Stopped algorithm after three hours runtime.
    \item[\dag] The authors of~\citeonly{C:BeiKraLea16,FSE:LiWan16} did not only give one matrix,
                but instead whole classes of \MDSs/ matrices.
                For~\citeonly{C:BeiKraLea16}, we chose the canonical candidate from its class.
                For~\citeonly{FSE:LiWan16}, we chose the matrix presented as an example in the paper.
        \vspace{0.75em}
    \item[1] Reported by~\citeonly{ToSC:JPST17}.
    \end{tablenotes}
    \end{threeparttable}
    \end{sidecaption}
\end{table}

\begin{fullwidth}
\begin{table}
    \caption[Comparison of matrices used in ciphers or hash functions]{%
        Matrices used in ciphers or hash functions.
        Note that matrices in the lower part of the table, marked with $^{\|}$, are not \MDSs/.
        Additionally these matrices are commonly not a target for \enquote{XOR count}-based implementation optimisations, as they are per se very efficiently implementable.
        The matrix constructions are Circulant~(C), Hadamard~(H), Involutory~(I), and Subfield~(S).
    }\label{slps:tab:ciphers}
    \centering
    \begin{threeparttable}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{llrrrrr}
        \toprule
        Cipher                                                                                       &             Type                               & Na\"ive &     Literature     & \textsc{Paar1} & \textsc{Paar2} &      \textsc{BP}      \\
        \midrule
        \AES/~\citeonly{rijndael_book}\tnote{\ddag}\hphantom{\ddag} (C)      & $\parens{\F_2[x]/\mathtt{0x11b}}^{4 \times 4}$ &  152  & $  7+ 96$\tnote{1} &      108       &   108\tnote{*} &  97\tnote{\dag} \\ \rowcolor{gray!10}
        \anubis/~\citeonly{NESSIE:anubis} (H, I)                             & $\parens{\F_2[x]/\mathtt{0x11d}}^{4 \times 4}$ &  184  & $ 80+ 96$\tnote{2} &      122       &   121\tnote{*} & 113           \\
        \clefia/ $\mathrm{M}_0$~\citeonly{FSE:SSAMI07} (H)                   & $\parens{\F_2[x]/\mathtt{0x11d}}^{4 \times 4}$ &  184  &       ---\tnote{5} &      121       &   121\tnote{*} & 106           \\ \rowcolor{gray!10}
        \clefia/ $\mathrm{M}_1$~\citeonly{FSE:SSAMI07} (H)                   & $\parens{\F_2[x]/\mathtt{0x11d}}^{4 \times 4}$ &  208  &       ---\tnote{5} &      121       &   121\tnote{*} & 111           \\
        \fox/ \textsc{mu4}~\citeonly{SAC:JunVau04b}                          & $\parens{\F_2[x]/\mathtt{0x11b}}^{4 \times 4}$ &  219  &       ---\tnote{5} &      144       &   143\tnote{*} & 137           \\ \rowcolor{gray!10}
        \twofish/~\citeonly{AES:twofish}                                     & $\parens{\F_2[x]/\mathtt{0x169}}^{4 \times 4}$ &  327  &       ---\tnote{5} &      151       &   149\tnote{*} & 129           \\
        \midrule
        \fox/ \textsc{mu8}~\citeonly{SAC:JunVau04b}                          & $\parens{\F_2[x]/\mathtt{0x11b}}^{8 \times 8}$ & 1257  &       ---\tnote{5} &      611       &         ---    &      594      \\ \rowcolor{gray!10}
        \grostl/~\citeonly{SHA3:grostl} (C)                                  & $\parens{\F_2[x]/\mathtt{0x11b}}^{8 \times 8}$ & 1112  & $504+448$\tnote{2} &      493       &         ---    &      475      \\ %\rowcolor{gray!10}
        \khazad/~\citeonly{NESSIE:khazad} (H, I)                             & $\parens{\F_2[x]/\mathtt{0x11d}}^{8 \times 8}$ & 1232  & $584+448$\tnote{2} &      488       &         ---    &      507      \\ \rowcolor{gray!10}
        \whirlpool/~\citeonly{NESSIE:whirlpool}\tnote{\S}\hphantom{\S} (C)   & $\parens{\F_2[x]/\mathtt{0x11d}}^{8 \times 8}$ &  840  & $304+448$\tnote{2} &      481       &         ---    &      465      \\ %\rowcolor{gray!10}
        \midrule
        \joltik/~\citeonly{CAESAR:joltik} (H, I)                             & $\parens{\F_2[x]/\mathtt{0x13}}^{4 \times 4}$  &   72  & $ 20+ 48$\tnote{2} &       52       &          48    &       48      \\ \rowcolor{gray!10}
        \smallscaleaes/~\citeonly{FSE:CidMurRob05} (C)                       & $\parens{\F_2[x]/\mathtt{0x13}}^{4 \times 4}$  &   72  &       ---\tnote{5} &       54       &          54    &       47      \\ %\rowcolor{gray!10}
        \midrule
        \whirlwind/ $\mathrm{M}_0$~\citeonly{DCC:BNNRT10} (H, S)             & $\parens{\F_2[x]/\mathtt{0x13}}^{8 \times 8}$  &  488  & $168+224$\tnote{2} &      218       &   218\tnote{*} &      212      \\ \rowcolor{gray!10}
        \whirlwind/ $\mathrm{M}_1$~\citeonly{DCC:BNNRT10} (H, S)             & $\parens{\F_2[x]/\mathtt{0x13}}^{8 \times 8}$  &  536  & $184+224$\tnote{2} &      246       &   244\tnote{*} &      235      \\ %\rowcolor{gray!10}
        \bottomrule
        \toprule
        \qarma/128~\citeonly{ToSC:Avanzi17}\tnote{$\|$}\hphantom{$\|$} (C)   & $\parens{\F_2[x]/\mathtt{0x101}}^{4 \times 4}$ &   64  &       ---\tnote{5} &       48       &          48    &       48      \\ \rowcolor{gray!10}
        \midrule
        \aria/~\citeonly{ICISC/KKPSSS03}\tnote{$\|$}\hphantom{$\|$} (I)      & $\parens{\F_2}^{128 \times 128}$               &  768  &       480\tnote{3} &      416       &         ---    &      ---      \\ %\rowcolor{gray!10}
        \midori/~\citeonly{AC:BBISHA15}\tnote{$\|$,\P}\hphantom{$\|$,\P} (C) & $\parens{\F_{2^4}}^{4 \times 4}$               &   32  &       ---\tnote{5} &       24       &          24    &       24      \\ \rowcolor{gray!10}
        \prince/ $\widehat{\mathrm{M}}_0$, $\widehat{\mathrm{M}}_1$~\citeonly{AC:BCGKKK12}\tnote{$\|$} & $\parens{\F_2}^{16 \times 16}$               &   32  &       ---\tnote{5} &       24       &          24    &       24      \\ %\rowcolor{gray!10}
        \pride/ $\mathrm{L}_0$--$\mathrm{L}_3$~\citeonly{C:ADKLPY14}\tnote{$\|$} & $\parens{\F_2}^{16 \times 16}$                 &   32  &       ---\tnote{5} &       24       &          24    &       24      \\ \rowcolor{gray!10}
        \qarma/64~\citeonly{ToSC:Avanzi17}\tnote{$\|$}\hphantom{$\|$} (C)    & $\parens{\F_2[x]/\mathtt{0x11}}^{4 \times 4}$  &   32  &       ---\tnote{5} &       24       &          24    &       24      \\ %\rowcolor{gray!10}
        \skinny/64~\citeonly{C:BJKLMP16}\tnote{$\|$}\hphantom{$\|$}          & $\parens{\F_{2^4}}^{4 \times 4}$               &   16  &        12\tnote{4} &       12       &          12    &       12      \\ %\rowcolor{gray!10}
        \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item[*] Stopped algorithm after three hours runtime.
    \item[\dag] For the implementation see our \textsc{GitHub} repository.
    \item[\ddag] Also used in other primitives, \eg/ its predecessor \square/~\citeonly{FSE:DaeKnuRij97}, and \mugi/~\citeonly{FSE:WFYTP02}.
    \item[\S] Also used in \maelstrom/~\citeonly{SBSEG:FilBarRij06}.
    \item[\P] Also used in other ciphers, \eg/ \mantis/~\citeonly{C:BJKLMP16}, and \fides/~\citeonly{CHES:BBKMW13}.
    \item[$\|$] Not an \MDSs/ matrix.
        \vspace{0.75em}
    \item[1] Reported by~\citeonly{CHES:JMPS17}.
    \item[2] Reported by~\citeonly{ToSC:JPST17}.
    \item[3] Reported by~\citeonly{ETRI:BCLOP04}.
    \item[4] Reported by the designers.
    \item[5] We are not aware of any reported results for this matrix.
    \end{tablenotes}
    \end{threeparttable}
\end{table}
\end{fullwidth}

\begin{figure}
\begin{sidecaption}[XOR count distributions for $4 \times 4$ \MDSs/ matrices over $\GL{4}$]{%
    XOR count distributions for $4 \times 4$ \MDSs/ matrix constructions over $\GL{4}$.
}[slps:fig:distributions]
    \centering
\begin{tikzpicture}
    \begin{axis}[
        no markers,
        xmin=35,
        xmax=165,
        ymin=0,
        %ymax=0.05,
        height=6cm,
        width=\textwidth,
        xlabel={Na\"ive implementation},
        yticklabels={},
        legend style={at={(0.5,1.4)}, anchor=north, font=\footnotesize, legend columns=3},
    ]
        \addplot+[color=RoyalBlue!75!Black, very thick, solid] table {slp/cauchy_4x4_4_naive.dat};
        \addlegendentry{Cauchy}
        \addplot+[color=yellow, very thick, solid] table {slp/circulant_left_4x4_4_naive.dat};
        \addlegendentry{Circulant}
        \addplot+[color=yellow, very thick, dashed] table {slp/enum_circulant_left_4x4_4_naive.dat};
        \addlegendentry{Circulant (enum.)}
        \addplot+[color=linkgreen, very thick, solid] table {slp/hadamard_4x4_4_naive.dat};
        \addlegendentry{Hadamard}
        \addplot+[color=linkgreen, very thick, dashed] table {slp/enum_hadamard_4x4_4_naive.dat};
        \addlegendentry{Hadamard (enum.)}
        \addplot+[color=orange, very thick, solid] table {slp/toeplitz_4x4_4_naive.dat};
        \addlegendentry{Toeplitz}
        \addplot+[color=orange, very thick, dashed] table {slp/enum_toeplitz_4x4_4_naive.dat};
        \addlegendentry{Toeplitz (enum.)}
        \addplot+[color=alertred, very thick, solid] table {slp/vandermonde_4x4_4_naive.dat};
        \addlegendentry{Vandermonde}
        \addplot+[color=black, very thick, dashed] table {slp/enum_4x4_4_naive.dat};
        \addlegendentry{Arbitrary (enum.)}
    \end{axis}
\end{tikzpicture}
\vspace{2em}

\begin{tikzpicture}
    \begin{axis}[
        no markers,
        xmin=35,
        xmax=85,
        ymin=0,
        %ymax=0.125,
        height=6cm,
        width=\textwidth,
        xlabel={\textsc{Paar1} implementation},
        yticklabels={},
        %legend pos=outer north east,
    ]
        \addplot+[color=RoyalBlue!75!Black, very thick, solid] table {slp/cauchy_4x4_4_paar1.dat};
        %\addlegendentry{Cauchy}
        \addplot+[color=yellow, very thick, solid] table {slp/circulant_left_4x4_4_paar1.dat};
        %\addlegendentry{Circulant}
        \addplot+[color=yellow, very thick, dashed] table {slp/enum_circulant_left_4x4_4_paar1.dat};
        %\addlegendentry{Circulant (enum.)}
        \addplot+[color=linkgreen, very thick, solid] table {slp/hadamard_4x4_4_paar1.dat};
        %\addlegendentry{Hadamard}
        \addplot+[color=linkgreen, very thick, dashed] table {slp/enum_hadamard_4x4_4_paar1.dat};
        %\addlegendentry{Hadamard (enum.)}
        \addplot+[color=orange, very thick, solid] table {slp/toeplitz_4x4_4_paar1.dat};
        %\addlegendentry{Toeplitz}
        \addplot+[color=orange, very thick, dashed] table {slp/enum_toeplitz_4x4_4_paar1.dat};
        %\addlegendentry{Toeplitz (enum.)}
        \addplot+[color=alertred, very thick, solid] table {slp/vandermonde_4x4_4_paar1.dat};
        %\addlegendentry{Vandermonde}
        \addplot+[color=black, very thick, dashed] table {slp/enum_4x4_4_paar1.dat};
        %\addlegendentry{Arbitrary (enum.)}
    \end{axis}
\end{tikzpicture}
\vspace{2em}

\begin{tikzpicture}
    \begin{axis}[
        no markers,
        xmin=35,
        xmax=85,
        ymin=0,
        %ymax=0.125,
        height=6cm,
        width=\textwidth,
        xlabel={\textsc{BP} implementation},
        yticklabels={},
        %legend pos=outer north east,
    ]
        \addplot+[color=RoyalBlue!75!Black, very thick, solid] table {slp/cauchy_4x4_4_bmp.dat};
        %\addlegendentry{Cauchy}
        \addplot+[color=yellow, very thick, solid] table {slp/circulant_left_4x4_4_bmp.dat};
        %\addlegendentry{Circulant}
        \addplot+[color=yellow, very thick, dashed] table {slp/enum_circulant_left_4x4_4_bmp.dat};
        %\addlegendentry{Circulant (enum.)}
        \addplot+[color=linkgreen, very thick, solid] table {slp/hadamard_4x4_4_bmp.dat};
        %\addlegendentry{Hadamard}
        \addplot+[color=linkgreen, very thick, dashed] table {slp/enum_hadamard_4x4_4_bmp.dat};
        %\addlegendentry{Hadamard (enum.)}
        \addplot+[color=orange, very thick, solid] table {slp/toeplitz_4x4_4_bmp.dat};
        %\addlegendentry{Toeplitz}
        \addplot+[color=orange, very thick, dashed] table {slp/enum_toeplitz_4x4_4_bmp.dat};
        %\addlegendentry{Toeplitz (enum.)}
        \addplot+[color=alertred, very thick, solid] table {slp/vandermonde_4x4_4_bmp.dat};
        %\addlegendentry{Vandermonde}
        \addplot+[color=black, very thick, dashed] table {slp/enum_4x4_4_bmp.dat};
        %\addlegendentry{Arbitrary (enum.)}
    \end{axis}
\end{tikzpicture}
\end{sidecaption}
\end{figure}
